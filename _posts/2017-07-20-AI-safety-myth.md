---
layout: post
title: 关于 AI 的若干迷思
category: AI
tags: BAI, AGI, ASI, Elon Musk
---

Elon Musk[^1] 最近又在谈 AI 对于人类的[潜在威胁](http://www.npr.org/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk)了，我并不否认这项议题的重要，但它容易让我们忽略 AI 更短期的影响，比如大规模自动化，以及对工作的替代。

在 AI 问题上，部分议题确有争论，就连顶尖科学家都没有达成共识，但也有很多议题属于不必要的争议，或者说是公众对其的迷思。[FLI](https://futureoflife.org/) 对此进行了很好地[概括](https://futureoflife.org/background/aimyths/)，我试译如下：

![Myth](/images/myths.jpg)

## 1. 时间推断上的迷思

第一个迷思与时间相关：机器需要多久能全面地超越人类智力？一个通常的错误观念是我们能精确地对此进行推断。

通常，人们误认为我们肯定能在本世纪内实现超级人工智能。事实上，人类历史上充满了对技术的高估。比如，从前人们认为一定会实现的聚变发电厂和飞行汽车，现如今却从未出现过。而 AI 在历史上，也一再地被高估，包括这个领域的先驱们。比如，John McCarthy（「人工智能」一词的提出者）、Marvin Minsky、Nathaniel Rochester 和 Claude Shannon 就曾写下过于乐观的预测，利用当时笨拙的电脑 2 个月时间就能完成：「我们计划进行 2 个月、10 人在达特茅斯学院的人工智能研究……试图了解怎样让机器使用语言，形成抽象和概念，解决目前只有人类能解决的问题，并让其自我改进。我们认为，只要有一组精心挑选的科学家一起工作一个夏天，就能在某些问题上取得重大进展。」

另一方面，与之相对的极端迷思是我们肯定不能在本世纪内实现超级人工智能。研究人员就我们离超级人工智能的时间差距做了广泛的估计，但我们没法确定这一概率在本世纪内为 0，因为过去种种对科技持怀疑的预测往往都是错的。比如，当时伟大的核物理学家 Ernest Rutherford 曾在 1933 年断言，利用核能是「妄想」(moonshine)，但不到 24 小时，Szilard 就发明了链式反应。另外，皇家天文学家 Richard Woolley 在 1956 年称星际穿越是「胡说」(utter bilge)。关于 AI 最为极端的迷思是超级人工智能永远不会实现， 因其在物理上就是不可能的。然而，物理学家们知道，大脑是由夸克和电子组成的强大计算机，没有任何定律规定了不能由其构成更为强大的智能。

当前已经有一系列调研询问 AI 研究者他们认为需要多少年实现人类级别的 AI 能达到 50% 的概率。所有这些调研得到了同样的结论，顶尖科学家都无法达成共识，更不要说我们了。比如，在 2015 年的波多黎各的 AI 大会上，平均（中位数）是 2045 年，但也有学者预测需要几百年。

还有一个相关的迷思是人们担心超级人工智能在几年内就会实现。事实上，多数人对超级人工智能的预测都发生在几十年以后。但他们认为，只要我们不是 100％ 确定超级人工智能不会在本世纪内发生，最聪明的做法就是现在便开始进行安全研究，以防范于未然。许多与人类级别 AI 相关的安全问题非常难，可能需要几十年才能解决。所以，与其等到一群喝着红牛的程序员不小心启动问题前才做出对策，现在就开始针对这些安全问题进行研究是明智的。



## 2. 争议的迷思

另一个常见的误解是，那些对 AI 感到担忧，提倡 AI 安全研究的都是些不懂 AI 的卢德分子([Luddite](https://en.wikipedia.org/wiki/Luddite))，当 Stuart Russell，[AI 标准教科书](https://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597)的作者在其波多黎各的演讲中举出这个例子时，观众们大笑不已。与之相关的一个误解是，支持 AI 安全研究有巨大争议。事实上，对 AI 安全研究的适当资助，并不意味着其风险很高，只是因为其不容忽视。就像为房屋购买适当的保险一样，是因其被烧毁的可能性不容忽视。

看起来，媒体使得 AI 安全问题看起来比实际上更具争议。毕竟，恐惧有市场，用夸张引言宣扬迫近的危机比言辞谨慎的平衡报告更容易引来点击率。因此，借由媒体来了解相互立场的两人，可能会认为他们的分歧比实际上更为严重。比如，技术怀疑主义者读了 Bill Gates 在英国小报中表达的观点，或许会误认为 Gates 相信超级人工智能即将来临。类似的，beneficial-AI（有益的人工智能） 运动中的某些人在对 Andrew Ng 的立场不太了解前，就因为他对火星人口过多的发言而错误地认为他不在乎 AI 安全问题，而事实上，他确有关心。关键点在于，Andrew 对时间的推断较长，因此他更倾向于优先解决短期内而非长期中面临的挑战。



## 3. 对超级人工智能风险的迷思

许多 AI 研究者看到这个[头条](http://www.dailymail.co.uk/sciencetech/article-2618434/Artificial-intelligence-worst-thing-happen-humanity-Stephen-Hawking-warns-rise-robots-disastrous-mankind.html)时都不以为然：「Stephen Hawking [^1]警告机器人的崛起可能给人类带来灾难。」因为许多人已经看过很多类似的文章了。典型的，这些文章会描述携带武器的邪恶机器人，并建议我们应该对机器人的崛起以及杀戮人类表示担忧，因为它们已经变得有意识和/或邪恶。乐观地看，这些文章其实相当有吸引力，因为他们简洁地总结了 AI 研究者**从不**担心的问题。这一场景结合了三种独立的误解：关于**意识**、**邪恶**和**机器人**。

如果你行驶在路上，你会对颜色、声音等有主观体验。但一辆自动驾驶汽车会有主观体验吗？它是否会觉得自己就像一部自动驾驶汽车？尽管这种关于意识的神秘本身很有趣，但它与 AI 的风险无关。如果你被无人车撞到，它是否拥有主观意识对你没有任何区别。同样，对我们人类的影响来自超级人工智能**做了**什么，而不是它的主观**感受**。

对机器人变得邪恶的恐惧是另一个无关话题(red herring)。真正需要担忧的不是恶意，而是能力。根据定义，超级人工智能非常善于实现其目标，无论这些目标是什么，所以我们需要保证它的目标和我们人类的目标一致。人类并不是都讨厌蚂蚁，但我们比它们更智能，所以如果我们想修建一座水电站但有一个蚁丘在哪，这对蚂蚁来说就不太妙了。beneficial-AI 运动希望避免人类被置于那些蚂蚁的处境中。

对于意识的误解与机器不能拥有目标的迷思相关。机器显然在狭义上拥有实现其目标行为的目标：热追踪导弹的目标可以从经济上解释为击中其靶位。如果你感觉受到了来自和你目标不一致的机器的威胁，那么真正困扰你的是它被设置的狭义的任务。如果那个热追踪导弹正追逐着你，你大概就不会惊叫：「我并不担心，因为机器本身没有目标！」

我同情 Rodney Brooks 以及其他被小报不公平地妖魔化的机器人先驱，因为一些记者过分痴迷于机器人，并且在很多文章中用闪着红色双眼的邪恶金属怪物来刻画它们。事实上，beneficial-AI 运动关注的并不是机器人，而是智能本身：特别是那些与我们目标不一致的智能。若要给我们带来麻烦，这些超级人工智能其实并不需要机器人的躯体，而只需网络连接即可，如引发超出控制的金融市场，取代人类研究者的创新能力，超越人类领导人的掌控力，以及开发出我们甚至都不能理解的武器等。就算建造机器人在物理上是不可行的，一个超智能、超富有的 AI 可以轻易用金钱支付或是操纵许多人类无意识地听命于它的指示。

对机器人的误解与机器不能控制人类的迷思相关。智能可以实现控制：人类可以控制老虎并不是因为我们更强壮，而是因为我们更聪明。这意味着如果我们从我们星球上智慧巅峰的地位上退去，我们有可能也失去了控制力。



## 4. 有趣的争议

与其把时间浪费在上述误解中，不如让我们聚焦于那些真实而有趣的，就连专家们都无法形成共识的争议上。你希望拥有什么样的未来？我们是否应该开发致命性的自动武器？你希望通过工作自动化带来什么？你将会给当今的孩子传递什么职业建议？你是喜欢一个新工作替代旧工作的社会，还是一个无需工作人人皆可享受生活而由机器来创造财富的社会？不久的将来，你是否希望我们创造出超级智能的生命并将其传播到我们的宇宙中？是我们来控制机器还是机器来控制我们？智能机器是会替代我们，还是与我们融合？在人工智能时代，作为人类意味着什么？你希望人类拥有什么样的意义，我们如何才能实现那样的未来？欢迎加入讨论！



---

[^1]: 译注：我觉得 Elon Musk 和 Stephen Hawking 有个共同点，他们都在以百年或是更长的时间尺度在思考。